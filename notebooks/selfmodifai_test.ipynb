{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jacksonkarel/selfmodifai/blob/main/notebooks/selfmodifai_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/tloen/alpaca-lora.git"
      ],
      "metadata": {
        "id": "sBHQLj9Gyya3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(\"alpaca-lora\")"
      ],
      "metadata": {
        "id": "x2HUy2mQz_GP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BAgfE8uJ5mf0"
      },
      "outputs": [],
      "source": [
        "!pip install -r requirements.txt\n",
        "!pip install langchain"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "import torch\n",
        "import transformers\n",
        "from peft import PeftModel\n",
        "from transformers import GenerationConfig, LlamaForCausalLM, LlamaTokenizer\n",
        "\n",
        "from utils.callbacks import Iteratorize, Stream\n",
        "from utils.prompter import Prompter\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "\n",
        "try:\n",
        "    if torch.backends.mps.is_available():\n",
        "        device = \"mps\"\n",
        "except:  # noqa: E722\n",
        "    pass\n",
        "\n",
        "\n",
        "def alpaca_generate(\n",
        "    instruction: str,\n",
        "    load_8bit: bool = False,\n",
        "    base_model: str = \"\",\n",
        "    lora_weights: str = \"tloen/alpaca-lora-7b\",\n",
        "    prompt_template: str = \"\",  # The prompt template to use, will default to alpaca.\n",
        "\n",
        "):\n",
        "    \"\"\"Modified from https://github.com/tloen/alpaca-lora/blob/main/generate.py\"\"\"\n",
        "    \n",
        "    base_model = base_model or os.environ.get(\"BASE_MODEL\", \"\")\n",
        "    assert (\n",
        "        base_model\n",
        "    ), \"Please specify a --base_model, e.g. --base_model='huggyllama/llama-7b'\"\n",
        "\n",
        "    prompter = Prompter(prompt_template)\n",
        "    tokenizer = LlamaTokenizer.from_pretrained(base_model)\n",
        "    if device == \"cuda\":\n",
        "        model = LlamaForCausalLM.from_pretrained(\n",
        "            base_model,\n",
        "            load_in_8bit=load_8bit,\n",
        "            torch_dtype=torch.float16,\n",
        "            device_map=\"auto\",\n",
        "        )\n",
        "        model = PeftModel.from_pretrained(\n",
        "            model,\n",
        "            lora_weights,\n",
        "            torch_dtype=torch.float16,\n",
        "        )\n",
        "    elif device == \"mps\":\n",
        "        model = LlamaForCausalLM.from_pretrained(\n",
        "            base_model,\n",
        "            device_map={\"\": device},\n",
        "            torch_dtype=torch.float16,\n",
        "        )\n",
        "        model = PeftModel.from_pretrained(\n",
        "            model,\n",
        "            lora_weights,\n",
        "            device_map={\"\": device},\n",
        "            torch_dtype=torch.float16,\n",
        "        )\n",
        "    else:\n",
        "        model = LlamaForCausalLM.from_pretrained(\n",
        "            base_model, device_map={\"\": device}, low_cpu_mem_usage=True\n",
        "        )\n",
        "        model = PeftModel.from_pretrained(\n",
        "            model,\n",
        "            lora_weights,\n",
        "            device_map={\"\": device},\n",
        "        )\n",
        "\n",
        "    # unwind broken decapoda-research config\n",
        "    model.config.pad_token_id = tokenizer.pad_token_id = 0  # unk\n",
        "    model.config.bos_token_id = 1\n",
        "    model.config.eos_token_id = 2\n",
        "\n",
        "    if not load_8bit:\n",
        "        model.half()  # seems to fix bugs for some users.\n",
        "\n",
        "    model.eval()\n",
        "    if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n",
        "        model = torch.compile(model)\n",
        "\n",
        "    def evaluate(\n",
        "        instruction,\n",
        "        input=None,\n",
        "        temperature=0.1,\n",
        "        top_p=0.75,\n",
        "        top_k=40,\n",
        "        num_beams=4,\n",
        "        max_new_tokens=128,\n",
        "        stream_output=False,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        prompt = prompter.generate_prompt(instruction, input)\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "        input_ids = inputs[\"input_ids\"].to(device)\n",
        "        generation_config = GenerationConfig(\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            top_k=top_k,\n",
        "            num_beams=num_beams,\n",
        "            **kwargs,\n",
        "        )\n",
        "\n",
        "        generate_params = {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"generation_config\": generation_config,\n",
        "            \"return_dict_in_generate\": True,\n",
        "            \"output_scores\": True,\n",
        "            \"max_new_tokens\": max_new_tokens,\n",
        "        }\n",
        "\n",
        "        if stream_output:\n",
        "            # Stream the reply 1 token at a time.\n",
        "            # This is based on the trick of using 'stopping_criteria' to create an iterator,\n",
        "            # from https://github.com/oobabooga/text-generation-webui/blob/ad37f396fc8bcbab90e11ecf17c56c97bfbd4a9c/modules/text_generation.py#L216-L243.\n",
        "\n",
        "            def generate_with_callback(callback=None, **kwargs):\n",
        "                kwargs.setdefault(\n",
        "                    \"stopping_criteria\", transformers.StoppingCriteriaList()\n",
        "                )\n",
        "                kwargs[\"stopping_criteria\"].append(\n",
        "                    Stream(callback_func=callback)\n",
        "                )\n",
        "                with torch.no_grad():\n",
        "                    model.generate(**kwargs)\n",
        "\n",
        "            def generate_with_streaming(**kwargs):\n",
        "                return Iteratorize(\n",
        "                    generate_with_callback, kwargs, callback=None\n",
        "                )\n",
        "\n",
        "            with generate_with_streaming(**generate_params) as generator:\n",
        "                for output in generator:\n",
        "                    # new_tokens = len(output) - len(input_ids[0])\n",
        "                    decoded_output = tokenizer.decode(output)\n",
        "\n",
        "                    if output[-1] in [tokenizer.eos_token_id]:\n",
        "                        break\n",
        "\n",
        "                    yield prompter.get_response(decoded_output)\n",
        "            return  # early return for stream_output\n",
        "\n",
        "        # Without streaming\n",
        "        with torch.no_grad():\n",
        "            generation_output = model.generate(\n",
        "                input_ids=input_ids,\n",
        "                generation_config=generation_config,\n",
        "                return_dict_in_generate=True,\n",
        "                output_scores=True,\n",
        "                max_new_tokens=max_new_tokens,\n",
        "            )\n",
        "        s = generation_output.sequences[0]\n",
        "        output = tokenizer.decode(s)\n",
        "        yield prompter.get_response(output)\n",
        "\n",
        "    return evaluate(instruction)"
      ],
      "metadata": {
        "id": "ZMD607YduNq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Any, List, Mapping, Optional\n",
        "\n",
        "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
        "from langchain.llms.base import LLM"
      ],
      "metadata": {
        "id": "xvWFNzrJDqhX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomLLM(LLM):\n",
        "            \n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        return \"custom\"\n",
        "    \n",
        "    def _call(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        stop: Optional[List[str]] = None,\n",
        "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
        "    ) -> str:\n",
        "    \n",
        "        alpaca_generator = alpaca_generate(instruction=prompt, load_8bit=True, base_model='decapoda-research/llama-7b-hf')\n",
        "        output = next(alpaca_generator)\n",
        "        return output"
      ],
      "metadata": {
        "id": "XjlIN1lzDre4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = CustomLLM()"
      ],
      "metadata": {
        "id": "qIO_1I9baV_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm._call(input)"
      ],
      "metadata": {
        "id": "S82ChbOXfw8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: Let's think step by step.\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
      ],
      "metadata": {
        "id": "anYaETVlqJK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import initialize_agent\n",
        "from langchain.agents import load_tools"
      ],
      "metadata": {
        "id": "rQjMtHoo5vcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tools = load_tools([\"python_repl\"], llm=llm)\n",
        "\n",
        "agent = initialize_agent(tools, llm, agent=\"zero-shot-react-description\", verbose=True, )\n",
        "\n",
        "input = \"You are the large language model Alpaca-LoRA. Your code is in the current directory. Modify your own code so that you are a more powerful language model.\""
      ],
      "metadata": {
        "id": "oK08UxA1oX0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent.run(input)"
      ],
      "metadata": {
        "id": "F8qLg-e-Bvip"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}