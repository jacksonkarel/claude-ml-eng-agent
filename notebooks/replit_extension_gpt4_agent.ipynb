{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jacksonkarel/selfmodifai/blob/main/notebooks/replit_extension_gpt4_agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rR_n3zgZIITk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aae4a678-b9fa-44ac-81dd-c0e95798076c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting openai\n",
            "  Downloading openai-0.27.8-py3-none-any.whl (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers\n",
            "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n",
            "Collecting aiohttp (from openai)\n",
            "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp->openai)\n",
            "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->openai)\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting yarl<2.0,>=1.0 (from aiohttp->openai)\n",
            "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp->openai)\n",
            "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp->openai)\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Installing collected packages: tokenizers, safetensors, multidict, frozenlist, async-timeout, yarl, huggingface-hub, aiosignal, transformers, aiohttp, openai\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 frozenlist-1.3.3 huggingface-hub-0.15.1 multidict-6.0.4 openai-0.27.8 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.30.2 yarl-1.9.2\n"
          ]
        }
      ],
      "source": [
        "!pip install openai transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5YO_QtbfFr4o",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "dedc162a-537a-40db-c213-acf0a87d3bcd"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-5e7e4b48-0d81-48fd-9af7-30b33c2df6de\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-5e7e4b48-0d81-48fd-9af7-30b33c2df6de\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving keys.json to keys.json\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "plRBVokoEtkf"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import openai\n",
        "\n",
        "keys = json.load(open('keys.json'))\n",
        "openai.api_key = keys['openai']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"GIT_EMAIL\"] = keys[\"git_email\"]\n",
        "os.environ[\"GIT_USERNAME\"] = keys[\"git_username\"]"
      ],
      "metadata": {
        "id": "C3Y86UhYXSCa"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "!rm -rf /root/.ssh\n",
        "\n",
        "!mkdir /root/.ssh\n",
        "\n",
        "!tar xvzf ssh.tar.gz\n",
        "\n",
        "!cp ssh-colab/* /root/.ssh && rm -rf ssh-colab && rm -rf ssh.tar.gz\n",
        "!chmod 700 .shh\n",
        "\n",
        "!touch /root/.ssh/known_hosts\n",
        "!ssh-keyscan github.com >> /root/.ssh/known_hosts\n",
        "!chmod 644 /root/.ssh/known_hosts\n",
        "\n",
        "!git config --global user.email GIT_EMAIL\n",
        "!git config --global user.name GIT_USERNAME\n",
        "!git clone git@github.com:jacksonkarel/selfmodifai-alpaca-lora.git\n",
        "os.chdir(\"selfmodifai-alpaca-lora\")"
      ],
      "metadata": {
        "id": "AWY-kZDGvGX1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "access_token = keys[\"github_access_token\"]\n",
        "\n",
        "!git config --global user.email GIT_EMAIL\n",
        "!git config --global user.name GIT_USERNAME\n",
        "\n",
        "os.system(f\"git clone https://{access_token}@github.com/jacksonkarel/selfmodifai-alpaca-lora.git\")\n",
        "os.chdir(\"selfmodifai-alpaca-lora\")"
      ],
      "metadata": {
        "id": "ntGyYoah42pr"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('messages.json') as json_file:\n",
        "    messages = json.load(json_file)\n",
        "\n",
        "messages.pop()\n",
        "\n",
        "with open('messages.json', 'w') as outfile:\n",
        "    json.dump(messages, outfile)"
      ],
      "metadata": {
        "id": "sQaqZ11EjgV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git restore messages.json"
      ],
      "metadata": {
        "id": "r3ZE2fitHSyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm messages.json"
      ],
      "metadata": {
        "id": "q6qMNh7cpZyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "YLVzeKezF2z2"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import json\n",
        "from transformers import pipeline\n",
        "from openai.error import InvalidRequestError\n",
        "\n",
        "def update_messages(content, role, messages):\n",
        "    new_message = {\"role\": role, \"content\": content}\n",
        "    messages.append(new_message)\n",
        "\n",
        "    print(f\"Step: {content}\")\n",
        "    return messages\n",
        "\n",
        "def format_nbl(non_bash_languages):\n",
        "    if len(non_bash_languages) == 1:\n",
        "        nbl_str = non_bash_languages[0].title()\n",
        "\n",
        "    if len(non_bash_languages) == 2:\n",
        "        first_lang = non_bash_languages[0]\n",
        "        sec_lang = non_bash_languages[1]\n",
        "\n",
        "        nbl_str = f\"{first_lang} and {sec_lang}\"\n",
        "\n",
        "    else:\n",
        "        nbl_str = \"\"\n",
        "\n",
        "        for nbl in non_bash_languages[:-1]:\n",
        "            nbl_str += f\"{nbl}, \"\n",
        "\n",
        "        last_lang = non_bash_languages[-1]\n",
        "        nbl_str += f\"and {last_lang}\"\n",
        "\n",
        "    return nbl_str\n",
        "\n",
        "def detect_non_bash_code(chatgpt_output):\n",
        "    # Pattern to match code blocks\n",
        "    pattern = r'```(\\w+)\\n(.*?)```'\n",
        "\n",
        "    matches = re.findall(pattern, chatgpt_output, re.DOTALL)\n",
        "\n",
        "    non_bash_languages = []\n",
        "\n",
        "    for nb_match in matches:\n",
        "        language = nb_match[0]\n",
        "\n",
        "        # Check if the language is not bash\n",
        "        if language.lower() != 'bash':\n",
        "            non_bash_languages.append(language)\n",
        "\n",
        "    return non_bash_languages\n",
        "\n",
        "\n",
        "def handle_too_long_context(messages):\n",
        "    print(\"too long context\")\n",
        "    messages = messages[:-1]\n",
        "    full_context = \"Condense the information from the following past conversation between us. Keep all of the information that is relevant to the task at hand and remove all that is not:\\n\"\n",
        "    for message in messages[1:]:\n",
        "        role = message[\"role\"]\n",
        "        content = message[\"content\"]\n",
        "\n",
        "        full_context += (f\"{role}: {content}\\n\\n\")\n",
        "\n",
        "    print(full_context)\n",
        "    system_turn = {'role': 'system', 'content': 'You are part of an agent that is modifying the code of a machine learning model. The agent is in the model directory. When you write bash commands they will be executed and the output will be sent back to you.'}\n",
        "\n",
        "    less_messages = [system_turn, {'role': 'user', 'content': full_context}]\n",
        "\n",
        "    try:\n",
        "        response = openai.ChatCompletion.create(\n",
        "                model=\"gpt-4\",\n",
        "                messages=less_messages\n",
        "                )\n",
        "        less_messages = [system_turn, {'role': 'assistant', 'content': response[\"choices\"][0][\"message\"][\"content\"]}]\n",
        "\n",
        "    except InvalidRequestError as e:\n",
        "            # Check if the error message matches the context length issue\n",
        "            if \"maximum context length\" in str(e):\n",
        "                 response, less_messages = handle_too_long_context(messages)\n",
        "\n",
        "            else:\n",
        "                # Re-raise the exception if it's not what we're looking for\n",
        "                raise e\n",
        "\n",
        "    return response, less_messages\n",
        "\n",
        "def gpt4_agent():\n",
        "    ls_stream = os.popen(\"ls\")\n",
        "    ls_output = ls_stream.read()\n",
        "    ls_prompt = f\"ls:\\n{ls_output}\"\n",
        "\n",
        "    messages = [{\"role\": \"system\", \"content\": \"You are part of an agent that is modifying the code of a machine learning model. The agent is in the model directory. When you write bash commands they will be executed and the output will be sent back to you.\"}, {\"role\": \"user\", \"content\": \"I'm using you inside of an agent. My code is parsing your outputs and using it to execute bash commands. Modify the model architecture so that the model is more powerful.\"}, {\"role\": \"user\", \"content\": \"Create bash commands that do that. Give me them one by one.\"}, {\"role\": \"assistant\", \"content\": \"Sure! Please run the following bash command, and it will provide useful information about the model.\\n\\nTo ensure the model files are available in the current directory, run:\\n```bash\\nls\\n```\"}, {\"role\": \"user\", \"content\": ls_prompt}]\n",
        "\n",
        "    while True:\n",
        "\n",
        "        try:\n",
        "            response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-4\",\n",
        "            messages=messages\n",
        "            )\n",
        "        except InvalidRequestError as e:\n",
        "            # Check if the error message matches the context length issue\n",
        "            if \"maximum context length\" in str(e):\n",
        "                response, messages = handle_too_long_context(messages)\n",
        "\n",
        "                with open('messages.json', 'w') as outfile:\n",
        "                    json.dump(messages, outfile)\n",
        "\n",
        "            else:\n",
        "                # Re-raise the exception if it's not what we're looking for\n",
        "                raise e\n",
        "\n",
        "        response_content = response[\"choices\"][0][\"message\"][\"content\"]\n",
        "\n",
        "        messages = update_messages(response_content, \"assistant\", messages)\n",
        "\n",
        "        # Define the regular expression pattern\n",
        "        pattern = r'```bash\\n(.*?)\\n```'\n",
        "\n",
        "        bash_matches = re.findall(pattern, response_content, re.DOTALL)\n",
        "\n",
        "        non_bash_languages = detect_non_bash_code(response_content)\n",
        "\n",
        "        bash_response = \"Create bash commands that do that. Give me them one by one.\"\n",
        "        if bash_matches:\n",
        "            content = \"\"\n",
        "        # matches is now a list of all bash commands in the string\n",
        "            for bash_command in bash_matches:\n",
        "                content += f\"{bash_command}:\\n\"\n",
        "                stream = os.popen(bash_command)\n",
        "\n",
        "                content += stream.read()\n",
        "\n",
        "\n",
        "            if non_bash_languages:\n",
        "                nbl_str = format_nbl(non_bash_languages)\n",
        "\n",
        "                content += f\"Those are the outputs from those bash commands. Can you write bash commands to implement the {nbl_str} code?\"\n",
        "\n",
        "            elif not content:\n",
        "                content = \"Ok, did that\"\n",
        "\n",
        "        elif non_bash_languages:\n",
        "            languages = format_nbl(non_bash_languages)\n",
        "            content = f\"Write bash commands to implement those changes in the {languages} files.\"\n",
        "\n",
        "        elif \"?\" not in response_content:\n",
        "            content = bash_response\n",
        "\n",
        "        else:\n",
        "            classifier = pipeline(\"zero-shot-classification\")\n",
        "            labels = [\"suggestion for what to do next\", \"inquisitive question\", \"asking somebody to do something\"]\n",
        "            results = classifier(sequences=response_content, candidate_labels=labels, hypothesis_template=\"This text is a {}\")\n",
        "\n",
        "            if results[\"labels\"][0] == \"suggestion for what to do next\":\n",
        "                content = bash_response\n",
        "\n",
        "            else:\n",
        "                content = \"My goal is to make this machine learning model more powerful by improving its architecture. Find the answer to that question in that context. If you can't, try another step in improving the model.\"\n",
        "\n",
        "\n",
        "        messages = update_messages(content, \"user\", messages)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpt4_agent()"
      ],
      "metadata": {
        "id": "PDMCx-8JTmCa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "af5bcc03-634a-4a48-d83e-e15636762ae8"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step: Thanks for providing the list of files. However, it seems that the model files and architecture information are not immediately present in the given directory. To modify the model architecture, you'll need to locate and access the model's source code files, which are likely inside some of the directories listed here.\n",
            "\n",
            "Please run the following command to search for Python files in the subdirectories:\n",
            "\n",
            "```bash\n",
            "find . -name \"*.py\"\n",
            "```\n",
            "Step: find . -name \"*.py\":\n",
            "./utils/callbacks.py\n",
            "./utils/prompter.py\n",
            "./utils/__init__.py\n",
            "./export_hf_checkpoint.py\n",
            "./export_state_dict_checkpoint.py\n",
            "./finetune.py\n",
            "./generate.py\n",
            "\n",
            "Step: The `finetune.py` file seems to be where the model is likely being defined and trained. To modify the model's architecture to make it more powerful, you may need to modify this file.\n",
            "\n",
            "1. First, open the `finetune.py` file to view its contents:\n",
            "\n",
            "```bash\n",
            "cat finetune.py\n",
            "```\n",
            "\n",
            "2. Secondly, you can search for the lines of code in `finetune.py` where the model architecture is defined:\n",
            "\n",
            "```bash\n",
            "grep -n -C 3 -e \"model\" finetune.py\n",
            "```\n",
            "\n",
            "This command will find lines containing the word 'model' and show 3 lines of code around each occurrence. Once you find the line where the architecture is defined, you can note the line number and edit the file to make changes.\n",
            "\n",
            "3. To edit the `finetune.py` file:\n",
            "\n",
            "```bash\n",
            "nano finetune.py\n",
            "```\n",
            "\n",
            "Navigate to the line where the model definition is found and modify the architecture according to your requirements. Save the file (Ctrl + O) and exit nano (Ctrl + X).\n",
            "\n",
            "Keep in mind that I cannot parse or modify the script for you; you will have to adjust the model architecture yourself based on your domain knowledge. There are many ways to increase a model's power, such as adding more layers, widening layers, or even employing residual connections. Keep in mind that increasing model complexity might also impact overall training time and performance.\n",
            "Step: cat finetune.py:\n",
            "import os\n",
            "import sys\n",
            "from typing import List\n",
            "\n",
            "import fire\n",
            "import torch\n",
            "import transformers\n",
            "from datasets import load_dataset\n",
            "\n",
            "\"\"\"\n",
            "Unused imports:\n",
            "import torch.nn as nn\n",
            "import bitsandbytes as bnb\n",
            "\"\"\"\n",
            "\n",
            "from peft import (\n",
            "    LoraConfig,\n",
            "    get_peft_model,\n",
            "    get_peft_model_state_dict,\n",
            "    prepare_model_for_int8_training,\n",
            "    set_peft_model_state_dict,\n",
            ")\n",
            "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
            "\n",
            "from utils.prompter import Prompter\n",
            "\n",
            "\n",
            "def train(\n",
            "    # model/data params\n",
            "    base_model: str = \"\",  # the only required argument\n",
            "    data_path: str = \"yahma/alpaca-cleaned\",\n",
            "    output_dir: str = \"./lora-alpaca\",\n",
            "    # training hyperparams\n",
            "    batch_size: int = 128,\n",
            "    micro_batch_size: int = 4,\n",
            "    num_epochs: int = 3,\n",
            "    learning_rate: float = 3e-4,\n",
            "    cutoff_len: int = 256,\n",
            "    val_set_size: int = 2000,\n",
            "    # lora hyperparams\n",
            "    lora_r: int = 8,\n",
            "    lora_alpha: int = 16,\n",
            "    lora_dropout: float = 0.05,\n",
            "    lora_target_modules: List[str] = [\n",
            "        \"q_proj\",\n",
            "        \"v_proj\",\n",
            "    ],\n",
            "    # llm hyperparams\n",
            "    train_on_inputs: bool = True,  # if False, masks out inputs in loss\n",
            "    add_eos_token: bool = False,\n",
            "    group_by_length: bool = False,  # faster, but produces an odd training loss curve\n",
            "    # wandb params\n",
            "    wandb_project: str = \"\",\n",
            "    wandb_run_name: str = \"\",\n",
            "    wandb_watch: str = \"\",  # options: false | gradients | all\n",
            "    wandb_log_model: str = \"\",  # options: false | true\n",
            "    resume_from_checkpoint: str = None,  # either training checkpoint or final adapter\n",
            "    prompt_template_name: str = \"alpaca\",  # The prompt template to use, will default to alpaca.\n",
            "):\n",
            "    if int(os.environ.get(\"LOCAL_RANK\", 0)) == 0:\n",
            "        print(\n",
            "            f\"Training Alpaca-LoRA model with params:\\n\"\n",
            "            f\"base_model: {base_model}\\n\"\n",
            "            f\"data_path: {data_path}\\n\"\n",
            "            f\"output_dir: {output_dir}\\n\"\n",
            "            f\"batch_size: {batch_size}\\n\"\n",
            "            f\"micro_batch_size: {micro_batch_size}\\n\"\n",
            "            f\"num_epochs: {num_epochs}\\n\"\n",
            "            f\"learning_rate: {learning_rate}\\n\"\n",
            "            f\"cutoff_len: {cutoff_len}\\n\"\n",
            "            f\"val_set_size: {val_set_size}\\n\"\n",
            "            f\"lora_r: {lora_r}\\n\"\n",
            "            f\"lora_alpha: {lora_alpha}\\n\"\n",
            "            f\"lora_dropout: {lora_dropout}\\n\"\n",
            "            f\"lora_target_modules: {lora_target_modules}\\n\"\n",
            "            f\"train_on_inputs: {train_on_inputs}\\n\"\n",
            "            f\"add_eos_token: {add_eos_token}\\n\"\n",
            "            f\"group_by_length: {group_by_length}\\n\"\n",
            "            f\"wandb_project: {wandb_project}\\n\"\n",
            "            f\"wandb_run_name: {wandb_run_name}\\n\"\n",
            "            f\"wandb_watch: {wandb_watch}\\n\"\n",
            "            f\"wandb_log_model: {wandb_log_model}\\n\"\n",
            "            f\"resume_from_checkpoint: {resume_from_checkpoint or False}\\n\"\n",
            "            f\"prompt template: {prompt_template_name}\\n\"\n",
            "        )\n",
            "    assert (\n",
            "        base_model\n",
            "    ), \"Please specify a --base_model, e.g. --base_model='huggyllama/llama-7b'\"\n",
            "    gradient_accumulation_steps = batch_size // micro_batch_size\n",
            "\n",
            "    prompter = Prompter(prompt_template_name)\n",
            "\n",
            "    device_map = \"auto\"\n",
            "    world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n",
            "    ddp = world_size != 1\n",
            "    if ddp:\n",
            "        device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\") or 0)}\n",
            "        gradient_accumulation_steps = gradient_accumulation_steps // world_size\n",
            "\n",
            "    # Check if parameter passed or if set within environ\n",
            "    use_wandb = len(wandb_project) > 0 or (\n",
            "        \"WANDB_PROJECT\" in os.environ and len(os.environ[\"WANDB_PROJECT\"]) > 0\n",
            "    )\n",
            "    # Only overwrite environ if wandb param passed\n",
            "    if len(wandb_project) > 0:\n",
            "        os.environ[\"WANDB_PROJECT\"] = wandb_project\n",
            "    if len(wandb_watch) > 0:\n",
            "        os.environ[\"WANDB_WATCH\"] = wandb_watch\n",
            "    if len(wandb_log_model) > 0:\n",
            "        os.environ[\"WANDB_LOG_MODEL\"] = wandb_log_model\n",
            "\n",
            "    model = LlamaForCausalLM.from_pretrained(\n",
            "        base_model,\n",
            "        load_in_8bit=True,\n",
            "        torch_dtype=torch.float16,\n",
            "        device_map=device_map,\n",
            "    )\n",
            "\n",
            "    tokenizer = LlamaTokenizer.from_pretrained(base_model)\n",
            "\n",
            "    tokenizer.pad_token_id = (\n",
            "        0  # unk. we want this to be different from the eos token\n",
            "    )\n",
            "    tokenizer.padding_side = \"left\"  # Allow batched inference\n",
            "\n",
            "    def tokenize(prompt, add_eos_token=True):\n",
            "        # there's probably a way to do this with the tokenizer settings\n",
            "        # but again, gotta move fast\n",
            "        result = tokenizer(\n",
            "            prompt,\n",
            "            truncation=True,\n",
            "            max_length=cutoff_len,\n",
            "            padding=False,\n",
            "            return_tensors=None,\n",
            "        )\n",
            "        if (\n",
            "            result[\"input_ids\"][-1] != tokenizer.eos_token_id\n",
            "            and len(result[\"input_ids\"]) < cutoff_len\n",
            "            and add_eos_token\n",
            "        ):\n",
            "            result[\"input_ids\"].append(tokenizer.eos_token_id)\n",
            "            result[\"attention_mask\"].append(1)\n",
            "\n",
            "        result[\"labels\"] = result[\"input_ids\"].copy()\n",
            "\n",
            "        return result\n",
            "\n",
            "    def generate_and_tokenize_prompt(data_point):\n",
            "        full_prompt = prompter.generate_prompt(\n",
            "            data_point[\"instruction\"],\n",
            "            data_point[\"input\"],\n",
            "            data_point[\"output\"],\n",
            "        )\n",
            "        tokenized_full_prompt = tokenize(full_prompt)\n",
            "        if not train_on_inputs:\n",
            "            user_prompt = prompter.generate_prompt(\n",
            "                data_point[\"instruction\"], data_point[\"input\"]\n",
            "            )\n",
            "            tokenized_user_prompt = tokenize(\n",
            "                user_prompt, add_eos_token=add_eos_token\n",
            "            )\n",
            "            user_prompt_len = len(tokenized_user_prompt[\"input_ids\"])\n",
            "\n",
            "            if add_eos_token:\n",
            "                user_prompt_len -= 1\n",
            "\n",
            "            tokenized_full_prompt[\"labels\"] = [\n",
            "                -100\n",
            "            ] * user_prompt_len + tokenized_full_prompt[\"labels\"][\n",
            "                user_prompt_len:\n",
            "            ]  # could be sped up, probably\n",
            "        return tokenized_full_prompt\n",
            "\n",
            "    model = prepare_model_for_int8_training(model)\n",
            "\n",
            "    config = LoraConfig(\n",
            "        r=lora_r,\n",
            "        lora_alpha=lora_alpha,\n",
            "        target_modules=lora_target_modules,\n",
            "        lora_dropout=lora_dropout,\n",
            "        bias=\"none\",\n",
            "        task_type=\"CAUSAL_LM\",\n",
            "    )\n",
            "    model = get_peft_model(model, config)\n",
            "\n",
            "    if data_path.endswith(\".json\") or data_path.endswith(\".jsonl\"):\n",
            "        data = load_dataset(\"json\", data_files=data_path)\n",
            "    else:\n",
            "        data = load_dataset(data_path)\n",
            "\n",
            "    if resume_from_checkpoint:\n",
            "        # Check the available weights and load them\n",
            "        checkpoint_name = os.path.join(\n",
            "            resume_from_checkpoint, \"pytorch_model.bin\"\n",
            "        )  # Full checkpoint\n",
            "        if not os.path.exists(checkpoint_name):\n",
            "            checkpoint_name = os.path.join(\n",
            "                resume_from_checkpoint, \"adapter_model.bin\"\n",
            "            )  # only LoRA model - LoRA config above has to fit\n",
            "            resume_from_checkpoint = (\n",
            "                False  # So the trainer won't try loading its state\n",
            "            )\n",
            "        # The two files above have a different name depending on how they were saved, but are actually the same.\n",
            "        if os.path.exists(checkpoint_name):\n",
            "            print(f\"Restarting from {checkpoint_name}\")\n",
            "            adapters_weights = torch.load(checkpoint_name)\n",
            "            set_peft_model_state_dict(model, adapters_weights)\n",
            "        else:\n",
            "            print(f\"Checkpoint {checkpoint_name} not found\")\n",
            "\n",
            "    model.print_trainable_parameters()  # Be more transparent about the % of trainable params.\n",
            "\n",
            "    if val_set_size > 0:\n",
            "        train_val = data[\"train\"].train_test_split(\n",
            "            test_size=val_set_size, shuffle=True, seed=42\n",
            "        )\n",
            "        train_data = (\n",
            "            train_val[\"train\"].shuffle().map(generate_and_tokenize_prompt)\n",
            "        )\n",
            "        val_data = (\n",
            "            train_val[\"test\"].shuffle().map(generate_and_tokenize_prompt)\n",
            "        )\n",
            "    else:\n",
            "        train_data = data[\"train\"].shuffle().map(generate_and_tokenize_prompt)\n",
            "        val_data = None\n",
            "\n",
            "    if not ddp and torch.cuda.device_count() > 1:\n",
            "        # keeps Trainer from trying its own DataParallelism when more than 1 gpu is available\n",
            "        model.is_parallelizable = True\n",
            "        model.model_parallel = True\n",
            "\n",
            "    trainer = transformers.Trainer(\n",
            "        model=model,\n",
            "        train_dataset=train_data,\n",
            "        eval_dataset=val_data,\n",
            "        args=transformers.TrainingArguments(\n",
            "            per_device_train_batch_size=micro_batch_size,\n",
            "            gradient_accumulation_steps=gradient_accumulation_steps,\n",
            "            warmup_steps=100,\n",
            "            num_train_epochs=num_epochs,\n",
            "            learning_rate=learning_rate,\n",
            "            fp16=True,\n",
            "            logging_steps=10,\n",
            "            optim=\"adamw_torch\",\n",
            "            evaluation_strategy=\"steps\" if val_set_size > 0 else \"no\",\n",
            "            save_strategy=\"steps\",\n",
            "            eval_steps=200 if val_set_size > 0 else None,\n",
            "            save_steps=200,\n",
            "            output_dir=output_dir,\n",
            "            save_total_limit=3,\n",
            "            load_best_model_at_end=True if val_set_size > 0 else False,\n",
            "            ddp_find_unused_parameters=False if ddp else None,\n",
            "            group_by_length=group_by_length,\n",
            "            report_to=\"wandb\" if use_wandb else None,\n",
            "            run_name=wandb_run_name if use_wandb else None,\n",
            "        ),\n",
            "        data_collator=transformers.DataCollatorForSeq2Seq(\n",
            "            tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n",
            "        ),\n",
            "    )\n",
            "    model.config.use_cache = False\n",
            "\n",
            "    old_state_dict = model.state_dict\n",
            "    model.state_dict = (\n",
            "        lambda self, *_, **__: get_peft_model_state_dict(\n",
            "            self, old_state_dict()\n",
            "        )\n",
            "    ).__get__(model, type(model))\n",
            "\n",
            "    if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n",
            "        model = torch.compile(model)\n",
            "\n",
            "    trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
            "\n",
            "    model.save_pretrained(output_dir)\n",
            "\n",
            "    print(\n",
            "        \"\\n If there's a warning about missing keys above, please disregard :)\"\n",
            "    )\n",
            "\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    fire.Fire(train)\n",
            "grep -n -C 3 -e \"model\" finetune.py:\n",
            "15-\n",
            "16-from peft import (\n",
            "17-    LoraConfig,\n",
            "18:    get_peft_model,\n",
            "19:    get_peft_model_state_dict,\n",
            "20:    prepare_model_for_int8_training,\n",
            "21:    set_peft_model_state_dict,\n",
            "22-)\n",
            "23-from transformers import LlamaForCausalLM, LlamaTokenizer\n",
            "24-\n",
            "--\n",
            "26-\n",
            "27-\n",
            "28-def train(\n",
            "29:    # model/data params\n",
            "30:    base_model: str = \"\",  # the only required argument\n",
            "31-    data_path: str = \"yahma/alpaca-cleaned\",\n",
            "32-    output_dir: str = \"./lora-alpaca\",\n",
            "33-    # training hyperparams\n",
            "--\n",
            "53-    wandb_project: str = \"\",\n",
            "54-    wandb_run_name: str = \"\",\n",
            "55-    wandb_watch: str = \"\",  # options: false | gradients | all\n",
            "56:    wandb_log_model: str = \"\",  # options: false | true\n",
            "57-    resume_from_checkpoint: str = None,  # either training checkpoint or final adapter\n",
            "58-    prompt_template_name: str = \"alpaca\",  # The prompt template to use, will default to alpaca.\n",
            "59-):\n",
            "60-    if int(os.environ.get(\"LOCAL_RANK\", 0)) == 0:\n",
            "61-        print(\n",
            "62:            f\"Training Alpaca-LoRA model with params:\\n\"\n",
            "63:            f\"base_model: {base_model}\\n\"\n",
            "64-            f\"data_path: {data_path}\\n\"\n",
            "65-            f\"output_dir: {output_dir}\\n\"\n",
            "66-            f\"batch_size: {batch_size}\\n\"\n",
            "--\n",
            "79-            f\"wandb_project: {wandb_project}\\n\"\n",
            "80-            f\"wandb_run_name: {wandb_run_name}\\n\"\n",
            "81-            f\"wandb_watch: {wandb_watch}\\n\"\n",
            "82:            f\"wandb_log_model: {wandb_log_model}\\n\"\n",
            "83-            f\"resume_from_checkpoint: {resume_from_checkpoint or False}\\n\"\n",
            "84-            f\"prompt template: {prompt_template_name}\\n\"\n",
            "85-        )\n",
            "86-    assert (\n",
            "87:        base_model\n",
            "88:    ), \"Please specify a --base_model, e.g. --base_model='huggyllama/llama-7b'\"\n",
            "89-    gradient_accumulation_steps = batch_size // micro_batch_size\n",
            "90-\n",
            "91-    prompter = Prompter(prompt_template_name)\n",
            "--\n",
            "106-        os.environ[\"WANDB_PROJECT\"] = wandb_project\n",
            "107-    if len(wandb_watch) > 0:\n",
            "108-        os.environ[\"WANDB_WATCH\"] = wandb_watch\n",
            "109:    if len(wandb_log_model) > 0:\n",
            "110:        os.environ[\"WANDB_LOG_MODEL\"] = wandb_log_model\n",
            "111-\n",
            "112:    model = LlamaForCausalLM.from_pretrained(\n",
            "113:        base_model,\n",
            "114-        load_in_8bit=True,\n",
            "115-        torch_dtype=torch.float16,\n",
            "116-        device_map=device_map,\n",
            "117-    )\n",
            "118-\n",
            "119:    tokenizer = LlamaTokenizer.from_pretrained(base_model)\n",
            "120-\n",
            "121-    tokenizer.pad_token_id = (\n",
            "122-        0  # unk. we want this to be different from the eos token\n",
            "--\n",
            "171-            ]  # could be sped up, probably\n",
            "172-        return tokenized_full_prompt\n",
            "173-\n",
            "174:    model = prepare_model_for_int8_training(model)\n",
            "175-\n",
            "176-    config = LoraConfig(\n",
            "177-        r=lora_r,\n",
            "--\n",
            "181-        bias=\"none\",\n",
            "182-        task_type=\"CAUSAL_LM\",\n",
            "183-    )\n",
            "184:    model = get_peft_model(model, config)\n",
            "185-\n",
            "186-    if data_path.endswith(\".json\") or data_path.endswith(\".jsonl\"):\n",
            "187-        data = load_dataset(\"json\", data_files=data_path)\n",
            "--\n",
            "191-    if resume_from_checkpoint:\n",
            "192-        # Check the available weights and load them\n",
            "193-        checkpoint_name = os.path.join(\n",
            "194:            resume_from_checkpoint, \"pytorch_model.bin\"\n",
            "195-        )  # Full checkpoint\n",
            "196-        if not os.path.exists(checkpoint_name):\n",
            "197-            checkpoint_name = os.path.join(\n",
            "198:                resume_from_checkpoint, \"adapter_model.bin\"\n",
            "199:            )  # only LoRA model - LoRA config above has to fit\n",
            "200-            resume_from_checkpoint = (\n",
            "201-                False  # So the trainer won't try loading its state\n",
            "202-            )\n",
            "--\n",
            "204-        if os.path.exists(checkpoint_name):\n",
            "205-            print(f\"Restarting from {checkpoint_name}\")\n",
            "206-            adapters_weights = torch.load(checkpoint_name)\n",
            "207:            set_peft_model_state_dict(model, adapters_weights)\n",
            "208-        else:\n",
            "209-            print(f\"Checkpoint {checkpoint_name} not found\")\n",
            "210-\n",
            "211:    model.print_trainable_parameters()  # Be more transparent about the % of trainable params.\n",
            "212-\n",
            "213-    if val_set_size > 0:\n",
            "214-        train_val = data[\"train\"].train_test_split(\n",
            "--\n",
            "226-\n",
            "227-    if not ddp and torch.cuda.device_count() > 1:\n",
            "228-        # keeps Trainer from trying its own DataParallelism when more than 1 gpu is available\n",
            "229:        model.is_parallelizable = True\n",
            "230:        model.model_parallel = True\n",
            "231-\n",
            "232-    trainer = transformers.Trainer(\n",
            "233:        model=model,\n",
            "234-        train_dataset=train_data,\n",
            "235-        eval_dataset=val_data,\n",
            "236-        args=transformers.TrainingArguments(\n",
            "--\n",
            "248-            save_steps=200,\n",
            "249-            output_dir=output_dir,\n",
            "250-            save_total_limit=3,\n",
            "251:            load_best_model_at_end=True if val_set_size > 0 else False,\n",
            "252-            ddp_find_unused_parameters=False if ddp else None,\n",
            "253-            group_by_length=group_by_length,\n",
            "254-            report_to=\"wandb\" if use_wandb else None,\n",
            "--\n",
            "258-            tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n",
            "259-        ),\n",
            "260-    )\n",
            "261:    model.config.use_cache = False\n",
            "262-\n",
            "263:    old_state_dict = model.state_dict\n",
            "264:    model.state_dict = (\n",
            "265:        lambda self, *_, **__: get_peft_model_state_dict(\n",
            "266-            self, old_state_dict()\n",
            "267-        )\n",
            "268:    ).__get__(model, type(model))\n",
            "269-\n",
            "270-    if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n",
            "271:        model = torch.compile(model)\n",
            "272-\n",
            "273-    trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
            "274-\n",
            "275:    model.save_pretrained(output_dir)\n",
            "276-\n",
            "277-    print(\n",
            "278-        \"\\n If there's a warning about missing keys above, please disregard :)\"\n",
            "nano finetune.py:\n",
            "\n",
            "Step: I have provided the content of `finetune.py` and the lines containing the word 'model.' You can edit the file using the `nano` text editor as described in the previous response:\n",
            "\n",
            "```bash\n",
            "nano finetune.py\n",
            "```\n",
            "\n",
            "Once you open the file in the editor, you can modify the architecture according to your requirements. Note that I am unable to access or modify the code directly since I can only provide guidance.\n",
            "Step: nano finetune.py:\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-4b102cf3d26e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgpt4_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-20-47a6d26ce07c>\u001b[0m in \u001b[0;36mgpt4_agent\u001b[0;34m()\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             response = openai.ChatCompletion.create(\n\u001b[0m\u001b[1;32m     96\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gpt-4\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/chat_completion.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTryAgain\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/abstract/engine_api_resource.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    151\u001b[0m         )\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         response, _, api_key = requestor.request(\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0mrequest_timeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m     ) -> Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], bool, str]:\n\u001b[0;32m--> 288\u001b[0;31m         result = self.request_raw(\n\u001b[0m\u001b[1;32m    289\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36mrequest_raw\u001b[0;34m(self, method, url, params, supplied_headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    594\u001b[0m             \u001b[0m_thread_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession_create_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m             result = _thread_context.session.request(\n\u001b[0m\u001b[1;32m    597\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m                 \u001b[0mabs_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    527\u001b[0m         }\n\u001b[1;32m    528\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 529\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 645\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    438\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mchunked\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m                 resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    441\u001b[0m                     \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m                     \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0;31m# Make the request on the httplib connection object.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m             httplib_response = self._make_request(\n\u001b[0m\u001b[1;32m    704\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    447\u001b[0m                     \u001b[0;31m# Python 3 (including for exceptions like SystemExit).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                     \u001b[0;31m# Otherwise it looks like a bug in the code.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSocketError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    442\u001b[0m                 \u001b[0;31m# Python 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m                     \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m                     \u001b[0;31m# Remove the TypeError from the exception chain in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1375\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1376\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1272\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1273\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1274\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1275\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1129\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1130\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZzZQHFITkX8a",
        "outputId": "8de083ca-bdae-48be-f78e-677eebb733ef"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "alpaca_data_cleaned_archive.json  generate.py\n",
            "alpaca_data_gpt4.json\t\t  lengths.ipynb\n",
            "alpaca_data.json\t\t  LICENSE\n",
            "DATA_LICENSE\t\t\t  messages.json\n",
            "docker-compose.yml\t\t  pyproject.toml\n",
            "Dockerfile\t\t\t  README.md\n",
            "export_hf_checkpoint.py\t\t  requirements.txt\n",
            "export_state_dict_checkpoint.py   templates\n",
            "finetune.py\t\t\t  utils\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPUowVrshiFlrovfQ57wQAT",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}