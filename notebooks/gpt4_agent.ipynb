{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jacksonkarel/selfmodifai/blob/main/notebooks/gpt4_agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rR_n3zgZIITk"
      },
      "outputs": [],
      "source": [
        "!pip install openai transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5YO_QtbfFr4o"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plRBVokoEtkf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import openai\n",
        "\n",
        "keys = json.load(open('keys.json'))\n",
        "openai.api_key = keys['openai']\n",
        "!rm keys.json\n",
        "\n",
        "!git lfs install\n",
        "!GIT_LFS_SKIP_SMUDGE=1\n",
        "!git clone https://huggingface.co/mosaicml/mpt-7b\n",
        "\n",
        "os.chdir(\"mpt-7b\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YLVzeKezF2z2"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from transformers import pipeline\n",
        "\n",
        "def gpt4_agent(multiple_commands=True):\n",
        "    messages = [{'role': 'system', 'content': 'You are part of an agent that is modifying the code of the model MPT-7b. The agent is in the MPT-7b directory. When you write code, that code will be executed and the output will be sent back to you.'}, {'role': 'user', 'content': \"I'm using you inside of an agent. My code is parsing your outputs and using it to execute code. Modify the code of MPT-7b so that it is a more powerful language model.\"}, {'role': 'assistant', 'content': \"As an AI language model, I don't have the ability to directly modify the code of other language models, including MPT-7b. However, I can suggest some potential improvements that can help enhance its performance: 1. Increase the size of the model: Bigger models can capture more complex patterns, which\"}, {'role': 'user', 'content': \"But I'm parsing your outputs\"}, {'role': 'assistant', 'content': 'I apologize for the confusion. In that case, let me provide you with some improvements in a code-like format. Please consider these proposed changes as high-level suggestions, bearing in mind that the implementation depends on the specific structure of the MPT-7b model you are working with.\\n\\nHere are the suggested improvements for MPT-7b:\\n\\n1. Increase model size\\n```python\\nMPT_7b_model.update_size(model_size=\\'larger\\')\\n```\\n\\n2. Use more training data\\n```python\\nMPT_7b_model.add_training_data(new_training_data)\\n```\\n\\n3. Fine-tuning on domain-specific data\\n```python\\nMPT_7b_model.fine_tune(domain_specific_data)\\n```\\n\\n4. Address bias using debiasing techniques\\n```python\\nMPT_7b_model.debias(method=\"bias_control\")\\n```'}, {'role': 'user', 'content': \"I can send you information about the model if you write some Bash commands, and I'll send the outputs to you\"}, {'role': 'assistant', 'content': \"Sure, I can provide you with some Bash commands to retrieve more information about the MPT-7b model. Here's a list of commands to gather useful details:\\n\\n1. List the files in the MPT-7b directory:\\n```bash\\nls\\n```\\n\\n2. Print the file containing the model architecture:\\n```bash\\ncat model_architecture.txt\\n```\\n\\n3. Display information about the training data:\\n```bash\\ncat\"}, {'role': 'user', 'content': 'Send me one message at a time'}, {'role': 'user', 'content': 'adapt_tokenizer.py\\nattention.py\\nblocks.py\\nconfig.json\\nconfiguration_mpt.py\\nflash_attn_triton.py\\ngeneration_config.json\\nhf_prefixlm_converter.py\\nmeta_init_context.py\\nmodeling_mpt.py\\nnorm.py\\nparam_init_fns.py\\npytorch_model-00001-of-00002.bin\\npytorch_model-00002-of-00002.bin\\npytorch_model.bin.index.json\\nREADME.md\\nspecial_tokens_map.json\\ntokenizer_config.json\\ntokenizer.json\\n'}, {'role': 'user', 'content': '\"\"\"A HuggingFace-style model configuration.\"\"\"\\nfrom typing import Dict, Optional, Union\\nfrom transformers import PretrainedConfig\\nattn_config_defaults: Dict = {\\'attn_type\\': \\'multihead_attention\\', \\'attn_pdrop\\': 0.0, \\'attn_impl\\': \\'triton\\', \\'qk_ln\\': False, \\'clip_qkv\\': None, \\'softmax_scale\\': None, \\'prefix_lm\\': False, \\'attn_uses_sequence_id\\': False, \\'alibi\\': False, \\'alibi_bias_max\\': 8}\\ninit_config_defaults: Dict = {\\'name\\': \\'kaiming_normal_\\', \\'fan_mode\\': \\'fan_in\\', \\'init_nonlinearity\\': \\'relu\\'}\\n\\nclass MPTConfig(PretrainedConfig):\\n    model_type = \\'mpt\\'\\n\\n    def __init__(self, d_model: int=2048, n_heads: int=16, n_layers: int=24, expansion_ratio: int=4, max_seq_len: int=2048, vocab_size: int=50368, resid_pdrop: float=0.0, emb_pdrop: float=0.0, learned_pos_emb: bool=True, attn_config: Dict=attn_config_defaults, init_device: str=\\'cpu\\', logit_scale: Optional[Union[float, str]]=None, no_bias: bool=False, verbose: int=0, embedding_fraction: float=1.0, norm_type: str=\\'low_precision_layernorm\\', use_cache: bool=False, init_config: Dict=init_config_defaults, **kwargs):\\n        \"\"\"The MPT configuration class.\\n\\n        Args:\\n            d_model (int): The size of the embedding dimension of the model.\\n            n_heads (int): The number of attention heads.\\n            n_layers (int): The number of layers in the model.\\n            expansion_ratio (int): The ratio of the up/down scale in the MLP.\\n            max_seq_len (int): The maximum sequence length of the model.\\n            vocab_size (int): The size of the vocabulary.\\n            resid_pdrop (float): The dropout probability applied to the attention output before combining with residual.\\n            emb_pdrop (float): The dropout probability for the embedding layer.\\n            learned_pos_emb (bool): Whether to use learned positional embeddings\\n            attn_config (Dict):  A dictionary used to configure the model\\'s attention module:\\n                attn_type (str): type of attention to use. Options: multihead_attention, multiquery_attention\\n                attn_pdrop (float): The dropout probability for the attention layers.\\n                attn_impl (str): The attention implementation to use. One of \\'torch\\', \\'flash\\', or \\'triton\\'.\\n                qk_ln (bool): Whether to apply layer normalization to the queries and keys in the attention layer.\\n                clip_qkv (Optional[float]): If not None, clip the queries, keys, and values in the attention layer to\\n                    this value.\\n                softmax_scale (Optional[float]): If not None, scale the softmax in the attention layer by this value. If None,\\n                    use the default scale of ``1/sqrt(d_keys)``.\\n                prefix_lm (Optional[bool]): Whether the model should operate as a Prefix LM. This requires passing an\\n                    extra `prefix_mask` argument which indicates which tokens belong to the prefix. Tokens in the prefix\\n                    can attend to one another bi-directionally. Tokens outside the prefix use causal attention.\\n                attn_uses_sequence_id (Optional[bool]): Whether to restrict attention to tokens that have the same sequence_id.\\n                    When the model is in `train` mode, this requires passing an extra `sequence_id` argument which indicates\\n                    which sub-sequence each token belongs to.\\n                    Defaults to ``False`` meaning any provided `sequence_id` will be ignored.\\n                alibi (bool): Whether to use the alibi bias instead of position embeddings.\\n                alibi_bias_max (int): The maximum value of the alibi bias.\\n            init_device (str): The device to use for parameter initialization.\\n            logit_scale (Optional[Union[float, str]]): If not None, scale the logits by this value.\\n            no_bias (bool): Whether to use bias in all layers.\\n            verbose (int): The verbosity level. 0 is silent.\\n            embedding_fraction (float): The fraction to scale the gradients of the embedding layer by.\\n            norm_type (str): choose type of norm to use\\n            multiquery_attention (bool): Whether to use multiquery attention implementation.\\n            use_cache (bool): Whether or not the model should return the last key/values attentions\\n            init_config (Dict): A dictionary used to configure the model initialization:\\n                init_config.name: The parameter initialization scheme to use. Options: \\'default_\\', \\'baseline_\\',\\n                    \\'kaiming_uniform_\\', \\'kaiming_normal_\\', \\'neox_init_\\', \\'small_init_\\', \\'xavier_uniform_\\', or\\n                    \\'xavier_normal_\\'. These mimic the parameter initialization methods in PyTorch.\\n                init_div_is_residual (Union[int, float, str, bool]): Value to divide initial weights by if ``module._is_residual`` is True.\\n                emb_init_std (Optional[float]): The standard deviation of the normal distribution used to initialize the embedding layer.\\n                emb_init_uniform_lim (Optional[Union[Tuple[float, float], float]]): The lower and upper limits of the uniform distribution\\n                    used to initialize the embedding layer. Mutually exclusive with ``emb_init_std``.\\n                init_std (float): The standard deviation of the normal distribution used to initialize the model,\\n                    if using the baseline_ parameter initialization scheme.\\n                init_gain (float): The gain to use for parameter initialization with kaiming or xavier initialization schemes.\\n                fan_mode (str): The fan mode to use for parameter initialization with kaiming initialization schemes.\\n                init_nonlinearity (str): The nonlinearity to use for parameter initialization with kaiming initialization schemes.\\n                ---\\n                See llmfoundry.models.utils.param_init_fns.py for info on other param init config options\\n        \"\"\"\\n        self.d_model = d_model\\n        self.n_heads = n_heads\\n        self.n_layers = n_layers\\n        self.expansion_ratio = expansion_ratio\\n        self.max_seq_len = max_seq_len\\n        self.vocab_size = vocab_size\\n        self.resid_pdrop = resid_pdrop\\n        self.emb_pdrop = emb_pdrop\\n        self.learned_pos_emb = learned_pos_emb\\n        self.attn_config = attn_config\\n        self.init_device = init_device\\n        self.logit_scale = logit_scale\\n        self.no_bias = no_bias\\n        self.verbose = verbose\\n        self.embedding_fraction = embedding_fraction\\n        self.norm_type = norm_type\\n        self.use_cache = use_cache\\n        self.init_config = init_config\\n        if \\'name\\' in kwargs:\\n            del kwargs[\\'name\\']\\n        if \\'loss_fn\\' in kwargs:\\n            del kwargs[\\'loss_fn\\']\\n        super().__init__(**kwargs)\\n        self._validate_config()\\n\\n    def _set_config_defaults(self, config, config_defaults):\\n        for (k, v) in config_defaults.items():\\n            if k not in config:\\n                config[k] = v\\n        return config\\n\\n    def _validate_config(self):\\n        self.attn_config = self._set_config_defaults(self.attn_config, attn_config_defaults)\\n        self.init_config = self._set_config_defaults(self.init_config, init_config_defaults)\\n        if self.d_model % self.n_heads != 0:\\n            raise ValueError(\\'d_model must be divisible by n_heads\\')\\n        if any((prob < 0 or prob > 1 for prob in [self.attn_config[\\'attn_pdrop\\'], self.resid_pdrop, self.emb_pdrop])):\\n            raise ValueError(\"self.attn_config[\\'attn_pdrop\\'], resid_pdrop, emb_pdrop are probabilities and must be between 0 and 1\")\\n        if self.attn_config[\\'attn_impl\\'] not in [\\'torch\\', \\'flash\\', \\'triton\\']:\\n            raise ValueError(f\"Unknown attn_impl={self.attn_config[\\'attn_impl\\']}\")\\n        if self.attn_config[\\'prefix_lm\\'] and self.attn_config[\\'attn_impl\\'] not in [\\'torch\\', \\'triton\\']:\\n            raise NotImplementedError(\\'prefix_lm only implemented with torch and triton attention.\\')\\n        if self.attn_config[\\'alibi\\'] and self.attn_config[\\'attn_impl\\'] not in [\\'torch\\', \\'triton\\']:\\n            raise NotImplementedError(\\'alibi only implemented with torch and triton attention.\\')\\n        if self.attn_config[\\'attn_uses_sequence_id\\'] and self.attn_config[\\'attn_impl\\'] not in [\\'torch\\', \\'triton\\']:\\n            raise NotImplementedError(\\'attn_uses_sequence_id only implemented with torch and triton attention.\\')\\n        if self.embedding_fraction > 1 or self.embedding_fraction <= 0:\\n            raise ValueError(\\'model.embedding_fraction must be between 0 (exclusive) and 1 (inclusive)!\\')\\n        if isinstance(self.logit_scale, str) and self.logit_scale != \\'inv_sqrt_d_model\\':\\n            raise ValueError(f\"self.logit_scale={self.logit_scale!r} is not recognized as an option; use numeric value or \\'inv_sqrt_d_model\\'.\")\\n        if self.init_config.get(\\'name\\', None) is None:\\n            raise ValueError(f\"self.init_config={self.init_config!r} \\'name\\' needs to be set.\")\\n        if not self.learned_pos_emb and (not self.attn_config[\\'alibi\\']):\\n            raise ValueError(f\\'Positional information must be provided to the model using either learned_pos_emb or alibi.\\')'},\n",
        "                {\"role\": \"user\", \"content\": \"Write a bash command to implement these changes\"}\n",
        "                ]\n",
        "\n",
        "    final_messages = []\n",
        "    finish_search = False\n",
        "\n",
        "    while finish_search is False:\n",
        "        response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-4\",\n",
        "        messages=messages\n",
        "        )\n",
        "\n",
        "        response_content = response[\"choices\"][0][\"message\"][\"content\"]\n",
        "        print(f\"Step: {response_content}\")\n",
        "        \n",
        "        # Define the regular expression pattern\n",
        "        pattern = r'```bash\\n(.*?)\\n```'\n",
        "\n",
        "        if multiple_commands:\n",
        "            matches = re.findall(pattern, response_content, re.DOTALL)\n",
        "\n",
        "            if matches:\n",
        "            # matches is now a list of all bash commands in the string\n",
        "                for bash_command in matches:\n",
        "                    stream = os.popen(bash_command)\n",
        "                    output = stream.read()\n",
        "                \n",
        "                content = \"Ok, did that\"\n",
        "                new_message = {\"role\": \"user\", \"content\": content}\n",
        "                messages.append(new_message)\n",
        "                print(f\"Step: {content}\")\n",
        "            \n",
        "            else:\n",
        "                classifier = pipeline(\"zero-shot-classification\")\n",
        "                labels = [\"suggestion for what to do next\", \"question\"]\n",
        "                results = classifier(sequences=text, candidate_labels=labels, hypothesis_template=\"This text is a\")\n",
        "\n",
        "                if results[\"scores\"][0] > 0.70:\n",
        "                    if results[\"labels\"][0] == \"suggestion for what to do next\": \n",
        "                        new_content = \"Create bash prompts that do that. Give me them one by one.\"\n",
        "                    \n",
        "                    else:\n",
        "                        new_content = \"My goal is to make MPT-7b a more powerful language model. Find the answer to that question in that context. If you can't, try another step in improving the language model.\"\n",
        "                \n",
        "                new_message = {\"role\": \"user\", \"content\": new_content}\n",
        "                messages.append(new_message)\n",
        "                \n",
        "\n",
        "                \n",
        "\n",
        "        else:\n",
        "            # Use re.search() to find the first match of the pattern in the string\n",
        "            re_match = re.search(pattern, response_content, re.DOTALL)\n",
        "\n",
        "            # The bash command will be in the first group of the match\n",
        "            if re_match:\n",
        "                bash_command = re_match.group(1)\n",
        "                stream = os.popen(bash_command)\n",
        "                output = stream.read()\n",
        "                print(f\"Step: {output}\")\n",
        "            \n",
        "                # new_message = {\"role\": \"user\", \"content\": output}\n",
        "                # messages.append(new_message)\n",
        "            \n",
        "            else:\n",
        "                final_messages = messages\n",
        "                finish_search = True\n",
        "\n",
        "gpt4_agent()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNtvRH3kzu5Ryd4YJE0dYWL",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}