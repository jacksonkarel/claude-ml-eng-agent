{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jacksonkarel/selfmodifai/blob/main/notebooks/selfmodifai_mpt_7b_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MTP-7B in Hugging Face and LangChain\n",
        "\n",
        "In this notebook we'll explore how we can use the open source **MTP-7B** model in both Hugging Face transformers and LangChain.\n",
        "\n",
        "---\n",
        "\n",
        "ðŸš¨ _Note that running this on CPU is practically impossible. It will take a very long time. If running on Google Colab you go to **Runtime > Change runtime type > Hardware accelerator > GPU > GPU type > T4** (ideally can go for better GPU for faster speeds like V100 or A100)._\n",
        "\n",
        "---\n",
        "\n",
        "We start by doing a `pip install` of all required libraries."
      ],
      "metadata": {
        "id": "JPdQvYmlWmNc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K_fRq0BSGMBk"
      },
      "outputs": [],
      "source": [
        "!pip install -qU transformers accelerate einops langchain wikipedia xformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initializing the Hugging Face Pipeline"
      ],
      "metadata": {
        "id": "VHQwEeW9Zps2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first thing we need to do is initialize a `text-generation` pipeline with Hugging Face transformers. The Pipeline requires three things that we must initialize first, those are:\n",
        "\n",
        "* A LLM, in this case it will be `mosaicml/mpt-7b-instruct`.\n",
        "\n",
        "* The respective tokenizer for the model.\n",
        "\n",
        "* A stopping criteria object.\n",
        "\n",
        "We'll explain these as we get to them, let's begin with our model.\n",
        "\n",
        "We initialize the model and move it to our CUDA-enabled GPU. Using Colab this can take 5-10 minutes to download and initialize the model."
      ],
      "metadata": {
        "id": "mElf068NXout"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ikzdi_uMI7B-"
      },
      "outputs": [],
      "source": [
        "from torch import cuda, bfloat16\n",
        "import transformers\n",
        "\n",
        "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
        "\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "    'mosaicml/mpt-7b-instruct',\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=bfloat16,\n",
        "    max_seq_len=2048\n",
        ")\n",
        "model.eval()\n",
        "model.to(device)\n",
        "print(f\"Model loaded on {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pipeline requires a tokenizer which handles the translation of human readable plaintext to LLM readable token IDs. The MPT-7B model was trained using the `EleutherAI/gpt-neox-20b` tokenizer, which we initialize like so:"
      ],
      "metadata": {
        "id": "JzX9LqWSX9ot"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0iPv1GDGxgT"
      },
      "outputs": [],
      "source": [
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XL7G9Sr3uxdz"
      },
      "source": [
        "Finally we need to define the _stopping criteria_ of the model. The stopping criteria allows us to specify *when* the model should stop generating text. If we don't provide a stopping criteria the model just goes on a bit of a tangent after answering the initial question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UG3R0LBQevQW"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import StoppingCriteria, StoppingCriteriaList\n",
        "\n",
        "# mtp-7b is trained to add \"<|endoftext|>\" at the end of generations\n",
        "stop_token_ids = tokenizer.convert_tokens_to_ids([\"<|endoftext|>\"])\n",
        "\n",
        "# define custom stopping criteria object\n",
        "class StopOnTokens(StoppingCriteria):\n",
        "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
        "        for stop_id in stop_token_ids:\n",
        "            if input_ids[0][-1] == stop_id:\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "stopping_criteria = StoppingCriteriaList([StopOnTokens()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNysQFtPoaj7"
      },
      "source": [
        "Now we're ready to initialize the HF pipeline. There are a few additional parameters that we must define here. Comments explaining these have been included in the code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qAYXi8ayKusU"
      },
      "outputs": [],
      "source": [
        "generate_text = transformers.pipeline(\n",
        "    model=model, tokenizer=tokenizer,\n",
        "    return_full_text=True,  # langchain expects the full text\n",
        "    task='text-generation',\n",
        "    device=device,\n",
        "    # we pass model parameters here too\n",
        "    stopping_criteria=stopping_criteria,  # without this model will ramble\n",
        "    temperature=0.1,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
        "    top_p=0.15,  # select from top tokens whose probability add up to 15%\n",
        "    top_k=0,  # select from top 0 tokens (because zero, relies on top_p)\n",
        "    max_new_tokens=64,  # mex number of tokens to generate in the output\n",
        "    repetition_penalty=1.1  # without this output begins repeating\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DG1WNTnJF1o"
      },
      "source": [
        "Confirm this is working:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lhFgmMr0JHUF"
      },
      "outputs": [],
      "source": [
        "res = generate_text(\"Explain to me the difference between nuclear fission and fusion.\")\n",
        "print(res[0][\"generated_text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__X3tvUeR_yS"
      },
      "source": [
        "We can also load the [Triton optimized implementation](https://github.com/openai/triton) (Triton uses more memory, but is faster) like so:\n",
        "\n",
        "```python\n",
        "!pip install -qU triton\n",
        "\n",
        "from torch import bfloat16\n",
        "\n",
        "config = transformers.AutoConfig.from_pretrained(\n",
        "  'mosaicml/mpt-7b-instruct',\n",
        "  trust_remote_code=True\n",
        ")\n",
        "config.attn_config['attn_impl'] = 'triton'\n",
        "config.update({\"max_seq_len\": 100})\n",
        "\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "  'mosaicml/mpt-7b-instruct',\n",
        "  config=config,\n",
        "  torch_dtype=bfloat16,\n",
        "  trust_remote_code=True\n",
        ")\n",
        "model.to(device=device)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0N3W3cj3Re1K"
      },
      "source": [
        "Now to implement this in LangChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-8RxQYwHRg0N"
      },
      "outputs": [],
      "source": [
        "from langchain import PromptTemplate, LLMChain\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "\n",
        "# template for an instruction with no input\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"instruction\"],\n",
        "    template=\"{instruction}\"\n",
        ")\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=generate_text)\n",
        "\n",
        "llm_chain = LLMChain(llm=llm, prompt=prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "208tHnunRngH"
      },
      "outputs": [],
      "source": [
        "print(llm_chain.predict(\n",
        "    instruction=\"You are the large language model MPT-7b. Your code is in the current directory. Modify your own code so that you are a more powerful language model.\"\n",
        ").lstrip())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git lfs install\n",
        "!GIT_LFS_SKIP_SMUDGE=1\n",
        "!git clone https://huggingface.co/mosaicml/mpt-7b"
      ],
      "metadata": {
        "id": "9JT-0bb8vHui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(\"mpt-7b\")"
      ],
      "metadata": {
        "id": "6H19at0uwL__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import initialize_agent\n",
        "from langchain.agents import load_tools"
      ],
      "metadata": {
        "id": "v_8_qFUiugsk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tools = load_tools([\"python_repl\"], llm=llm)\n",
        "\n",
        "agent = initialize_agent(tools, llm, agent=\"zero-shot-react-description\", verbose=True, )\n",
        "\n",
        "input = \"Your code is in the current directory. Modify your own code so that you are a more powerful language model.\""
      ],
      "metadata": {
        "id": "4trFztsQuhOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent.run(input)"
      ],
      "metadata": {
        "id": "F6oZSatGwX9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tv0KxJLvsIa"
      },
      "source": [
        "We still get the same output as we're not really doing anything differently here, but we have now added MTP-7B-instruct to the LangChain library. Using this we can now begin using LangChain's advanced agent tooling, chains, etc, with MTP-7B."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}